{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model for POTUS Speech Corpus using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/williamLi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stopwords from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS=['doc_id','date','pres','title','speech']\n",
    "df = pd.DataFrame(columns=COLUMNS)\n",
    "PRES = 'obama'\n",
    "pres_list = \"adams arthur bharrison buchanan bush carter cleveland clinton coolidge eisenhower fdroosevelt fillmore ford garfield grant gwbush harding harrison hayes hoover jackson jefferson johnson jqadams kennedy lbjohnson lincoln madison mckinley monroe nixon obama pierce polk reagan roosevelt taft taylor truman trump tyler vanburen washington wilson\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pres</th>\n",
       "      <th>title</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>August 10, 1927</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Address at the Opening of Work on Mount Rushmo...</td>\n",
       "      <td>We have come here to dedicate a cornerstone th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>December 8, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Third Annual Message</td>\n",
       "      <td>Members of the Congress: In meeting the consti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>December 6, 1923</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>First Annual Message</td>\n",
       "      <td>Since the close of the last Congress the Natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October 20, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Message Regarding Relationship of Church and S...</td>\n",
       "      <td>Mr. Moderator, Members Of The Council:\\nIt is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>March 4, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Inaugural Address</td>\n",
       "      <td>\\nMy Countrymen:\\n\\nNo one can contemplate cur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date      pres  \\\n",
       "doc_id                               \n",
       "1        August 10, 1927  coolidge   \n",
       "2       December 8, 1925  coolidge   \n",
       "3       December 6, 1923  coolidge   \n",
       "4       October 20, 1925  coolidge   \n",
       "5          March 4, 1925  coolidge   \n",
       "\n",
       "                                                    title  \\\n",
       "doc_id                                                      \n",
       "1       Address at the Opening of Work on Mount Rushmo...   \n",
       "2                                    Third Annual Message   \n",
       "3                                    First Annual Message   \n",
       "4       Message Regarding Relationship of Church and S...   \n",
       "5                                       Inaugural Address   \n",
       "\n",
       "                                                   speech  \n",
       "doc_id                                                     \n",
       "1       We have come here to dedicate a cornerstone th...  \n",
       "2       Members of the Congress: In meeting the consti...  \n",
       "3       Since the close of the last Congress the Natio...  \n",
       "4       Mr. Moderator, Members Of The Council:\\nIt is ...  \n",
       "5       \\nMy Countrymen:\\n\\nNo one can contemplate cur...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "_id = 1\n",
    "for filename in os.listdir('./speeches'):\n",
    "    if filename == '.DS_Store':\n",
    "        continue\n",
    "    for speech in os.listdir('./speeches/' + filename):\n",
    "        temp = open('./speeches/' + filename + '/' + speech, 'r', encoding='utf-8').readlines()\n",
    "        obj = {}\n",
    "        obj['doc_id'] = _id\n",
    "        date = re.findall('\"([^\"]*)\"', temp[1])\n",
    "        obj['date'] = date[0] if len(date) > 0 else None\n",
    "        obj['pres'] = filename\n",
    "        obj['title'] = re.findall('\"([^\"]*)\"', temp[0])[0]\n",
    "        obj['speech']= \"\".join(temp[2:])\n",
    "    \n",
    "        obj = pd.DataFrame(obj, index=[0])\n",
    "        df = df.append(obj, ignore_index=True)\n",
    "        _id += 1\n",
    "df = df.set_index(\"doc_id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pres</th>\n",
       "      <th>title</th>\n",
       "      <th>speech</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>August 10, 1927</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Address at the Opening of Work on Mount Rushmo...</td>\n",
       "      <td>We have come here to dedicate a cornerstone th...</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>December 8, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Third Annual Message</td>\n",
       "      <td>Members of the Congress: In meeting the consti...</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>December 6, 1923</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>First Annual Message</td>\n",
       "      <td>Since the close of the last Congress the Natio...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>October 20, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Message Regarding Relationship of Church and S...</td>\n",
       "      <td>Mr. Moderator, Members Of The Council:\\nIt is ...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>March 4, 1925</td>\n",
       "      <td>coolidge</td>\n",
       "      <td>Inaugural Address</td>\n",
       "      <td>\\nMy Countrymen:\\n\\nNo one can contemplate cur...</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date      pres  \\\n",
       "doc_id                               \n",
       "1        August 10, 1927  coolidge   \n",
       "2       December 8, 1925  coolidge   \n",
       "3       December 6, 1923  coolidge   \n",
       "4       October 20, 1925  coolidge   \n",
       "5          March 4, 1925  coolidge   \n",
       "\n",
       "                                                    title  \\\n",
       "doc_id                                                      \n",
       "1       Address at the Opening of Work on Mount Rushmo...   \n",
       "2                                    Third Annual Message   \n",
       "3                                    First Annual Message   \n",
       "4       Message Regarding Relationship of Church and S...   \n",
       "5                                       Inaugural Address   \n",
       "\n",
       "                                                   speech month day  year  \n",
       "doc_id                                                                     \n",
       "1       We have come here to dedicate a cornerstone th...    12   6  1925  \n",
       "2       Members of the Congress: In meeting the consti...    10  20  1923  \n",
       "3       Since the close of the last Congress the Natio...     3   4  1925  \n",
       "4       Mr. Moderator, Members Of The Council:\\nIt is ...     7   3  1925  \n",
       "5       \\nMy Countrymen:\\n\\nNo one can contemplate cur...     2  22  1925  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['month', 'day', 'year']] = pd.DataFrame([ str(x).split() for x in df['date'].tolist() ])\n",
    "months = ['', 'January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "              'August', 'September', 'October', 'November', 'December']\n",
    "df['month'] = pd.Series([ str(int(months.index(str(x)))) if str(x) in months else 0 for x in df['month'].tolist()])\n",
    "df['day'] = pd.Series([str(x)[0:len(str(x))-1] if x is not None else 0 for x in df['day'].tolist() ])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for a specific president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>pres</th>\n",
       "      <th>title</th>\n",
       "      <th>speech</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>May 15, 2016</td>\n",
       "      <td>obama</td>\n",
       "      <td>Commencement Address at Rutgers University</td>\n",
       "      <td>Hello Rutgers! &lt;Applause.&gt; R-U rah-rah! &lt;Appla...</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>obama</td>\n",
       "      <td>2016 State of the Union Address</td>\n",
       "      <td>Mr. Speaker, Mr. Vice President, Members of Co...</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>August 28, 2008</td>\n",
       "      <td>obama</td>\n",
       "      <td>Acceptance Speech at the Democratic National C...</td>\n",
       "      <td>To Chairman Dean and my great friend Dick Durb...</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>August 31, 2010</td>\n",
       "      <td>obama</td>\n",
       "      <td>Address on the End of the Combat Mission in Iraq</td>\n",
       "      <td>Good evening. Tonight, I'd like to talk to you...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>June 15, 2010</td>\n",
       "      <td>obama</td>\n",
       "      <td>Speech on the BP Oil Spill</td>\n",
       "      <td>Good evening. As we speak, our nation faces a ...</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date   pres  \\\n",
       "doc_id                            \n",
       "588         May 15, 2016  obama   \n",
       "589     January 12, 2016  obama   \n",
       "590      August 28, 2008  obama   \n",
       "591      August 31, 2010  obama   \n",
       "592        June 15, 2010  obama   \n",
       "\n",
       "                                                    title  \\\n",
       "doc_id                                                      \n",
       "588            Commencement Address at Rutgers University   \n",
       "589                       2016 State of the Union Address   \n",
       "590     Acceptance Speech at the Democratic National C...   \n",
       "591      Address on the End of the Combat Mission in Iraq   \n",
       "592                            Speech on the BP Oil Spill   \n",
       "\n",
       "                                                   speech month day  year  \n",
       "doc_id                                                                     \n",
       "588     Hello Rutgers! <Applause.> R-U rah-rah! <Appla...     8  28  2016  \n",
       "589     Mr. Speaker, Mr. Vice President, Members of Co...     8  31  2008  \n",
       "590     To Chairman Dean and my great friend Dick Durb...     6  15  2010  \n",
       "591     Good evening. Tonight, I'd like to talk to you...     1  29  2010  \n",
       "592     Good evening. As we speak, our nation faces a ...     3  15  2009  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df[df['pres'] == PRES]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = docs.speech.values.tolist()\n",
    "data = [re.sub('\\n+', ' ', sent) for sent in data]\n",
    "data = [re.sub(r\"\\<.*\\>\",\" \",sent) for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'rutgers']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'rutgers']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rutger']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('achieve', 4),\n",
       "  ('affection', 1),\n",
       "  ('afghanistan', 1),\n",
       "  ('ago', 1),\n",
       "  ('agree', 1),\n",
       "  ('ahead', 3),\n",
       "  ('alive', 1),\n",
       "  ('alliance', 1),\n",
       "  ('alone', 1),\n",
       "  ('already', 1),\n",
       "  ('always', 2),\n",
       "  ('america', 14),\n",
       "  ('american', 8),\n",
       "  ('ann', 2),\n",
       "  ('answer', 5),\n",
       "  ('anyone', 1),\n",
       "  ('anything', 1),\n",
       "  ('apathy', 1),\n",
       "  ('arc', 1),\n",
       "  ('arm', 1),\n",
       "  ('asian', 1),\n",
       "  ('ask', 2),\n",
       "  ('asleep', 1),\n",
       "  ('assemble', 1),\n",
       "  ('atlanta', 2),\n",
       "  ('autumn', 1),\n",
       "  ('awake', 1),\n",
       "  ('axelrod', 1),\n",
       "  ('back', 4),\n",
       "  ('backyard', 1),\n",
       "  ('bad', 1),\n",
       "  ('ballot', 2),\n",
       "  ('banner', 1),\n",
       "  ('beacon', 1),\n",
       "  ('bear', 1),\n",
       "  ('begin', 3),\n",
       "  ('believe', 1),\n",
       "  ('belong', 2),\n",
       "  ('bend', 1),\n",
       "  ('berlin', 1),\n",
       "  ('biden', 1),\n",
       "  ('bill', 1),\n",
       "  ('birmingham', 1),\n",
       "  ('bitter', 1),\n",
       "  ('black', 1),\n",
       "  ('block', 2),\n",
       "  ('blue', 1),\n",
       "  ('bomb', 1),\n",
       "  ('bond', 1),\n",
       "  ('bowl', 1),\n",
       "  ('brave', 3),\n",
       "  ('break', 1),\n",
       "  ('breathe', 1),\n",
       "  ('brick', 2),\n",
       "  ('bridge', 1),\n",
       "  ('bright', 1),\n",
       "  ('bring', 1),\n",
       "  ('build', 2),\n",
       "  ('burn', 1),\n",
       "  ('bus', 1),\n",
       "  ('call', 2),\n",
       "  ('callous', 1),\n",
       "  ('calloused', 1),\n",
       "  ('campaign', 5),\n",
       "  ('candidate', 1),\n",
       "  ('car', 1),\n",
       "  ('carry', 1),\n",
       "  ('cast', 2),\n",
       "  ('cause', 2),\n",
       "  ('celebrate', 1),\n",
       "  ('century', 4),\n",
       "  ('challenge', 2),\n",
       "  ('chance', 2),\n",
       "  ('change', 6),\n",
       "  ('charleston', 1),\n",
       "  ('chief', 1),\n",
       "  ('child', 2),\n",
       "  ('church', 1),\n",
       "  ('climb', 1),\n",
       "  ('cold', 1),\n",
       "  ('collection', 1),\n",
       "  ('college', 1),\n",
       "  ('color', 1),\n",
       "  ('come', 6),\n",
       "  ('common', 1),\n",
       "  ('concord', 1),\n",
       "  ('congratulate', 1),\n",
       "  ('connect', 1),\n",
       "  ('conquer', 1),\n",
       "  ('cooper', 2),\n",
       "  ('corner', 1),\n",
       "  ('could', 1),\n",
       "  ('country', 2),\n",
       "  ('create', 1),\n",
       "  ('creed', 2),\n",
       "  ('crisis', 2),\n",
       "  ('cynical', 1),\n",
       "  ('cynicism', 1),\n",
       "  ('dark', 1),\n",
       "  ('daughter', 1),\n",
       "  ('david', 2),\n",
       "  ('dawn', 1),\n",
       "  ('day', 2),\n",
       "  ('deal', 1),\n",
       "  ('debt', 1),\n",
       "  ('decision', 1),\n",
       "  ('defeat', 1),\n",
       "  ('define', 1),\n",
       "  ('delaware', 1),\n",
       "  ('democracy', 3),\n",
       "  ('democrat', 1),\n",
       "  ('democratic', 1),\n",
       "  ('depression', 1),\n",
       "  ('depth', 1),\n",
       "  ('des', 1),\n",
       "  ('desert', 1),\n",
       "  ('despair', 1),\n",
       "  ('destiny', 1),\n",
       "  ('determination', 1),\n",
       "  ('difference', 1),\n",
       "  ('different', 1),\n",
       "  ('dig', 1),\n",
       "  ('disabled', 2),\n",
       "  ('disagree', 1),\n",
       "  ('dismiss', 1),\n",
       "  ('divide', 2),\n",
       "  ('do', 2),\n",
       "  ('doctor', 1),\n",
       "  ('dollar', 3),\n",
       "  ('door', 2),\n",
       "  ('doubt', 2),\n",
       "  ('doubtful', 1),\n",
       "  ('dream', 2),\n",
       "  ('dust', 1),\n",
       "  ('earn', 2),\n",
       "  ('earth', 1),\n",
       "  ('elect', 1),\n",
       "  ('election', 5),\n",
       "  ('end', 1),\n",
       "  ('endorsement', 1),\n",
       "  ('endure', 2),\n",
       "  ('enemy', 1),\n",
       "  ('energy', 1),\n",
       "  ('enormity', 1),\n",
       "  ('enough', 1),\n",
       "  ('especially', 1),\n",
       "  ('even', 4),\n",
       "  ('ever', 1),\n",
       "  ('face', 1),\n",
       "  ('fall', 4),\n",
       "  ('false', 1),\n",
       "  ('family', 3),\n",
       "  ('far', 2),\n",
       "  ('father', 1),\n",
       "  ('fear', 1),\n",
       "  ('fearful', 1),\n",
       "  ('fight', 2),\n",
       "  ('financial', 2),\n",
       "  ('finger', 1),\n",
       "  ('first', 4),\n",
       "  ('forever', 1),\n",
       "  ('forget', 2),\n",
       "  ('forward', 1),\n",
       "  ('found', 1),\n",
       "  ('founder', 1),\n",
       "  ('friend', 2),\n",
       "  ('front', 1),\n",
       "  ('fundamental', 1),\n",
       "  ('gay', 1),\n",
       "  ('generation', 4),\n",
       "  ('genius', 1),\n",
       "  ('get', 4),\n",
       "  ('give', 2),\n",
       "  ('go', 1),\n",
       "  ('good', 3),\n",
       "  ('government', 2),\n",
       "  ('governor', 1),\n",
       "  ('gracious', 1),\n",
       "  ('grandmother', 1),\n",
       "  ('grateful', 1),\n",
       "  ('great', 2),\n",
       "  ('greatness', 1),\n",
       "  ('grow', 2),\n",
       "  ('hall', 1),\n",
       "  ('hand', 4),\n",
       "  ('happen', 3),\n",
       "  ('harbor', 1),\n",
       "  ('hard', 3),\n",
       "  ('harness', 1),\n",
       "  ('hatch', 1),\n",
       "  ('heal', 1),\n",
       "  ('hear', 2),\n",
       "  ('heart', 1),\n",
       "  ('heartache', 1),\n",
       "  ('heat', 1),\n",
       "  ('help', 1),\n",
       "  ('history', 2),\n",
       "  ('hold', 1),\n",
       "  ('home', 2),\n",
       "  ('honest', 1),\n",
       "  ('hope', 6),\n",
       "  ('hopeful', 1),\n",
       "  ('hose', 1),\n",
       "  ('hour', 3),\n",
       "  ('huddle', 1),\n",
       "  ('humility', 1),\n",
       "  ('ideal', 1),\n",
       "  ('imagination', 1),\n",
       "  ('imagine', 1),\n",
       "  ('immaturity', 1),\n",
       "  ('individual', 1),\n",
       "  ('iraq', 1),\n",
       "  ('job', 3),\n",
       "  ('joe', 1),\n",
       "  ('join', 1),\n",
       "  ('journey', 1),\n",
       "  ('kid', 1),\n",
       "  ('knock', 1),\n",
       "  ('know', 8),\n",
       "  ('lady', 1),\n",
       "  ('land', 1),\n",
       "  ('last', 1),\n",
       "  ('later', 1),\n",
       "  ('latino', 1),\n",
       "  ('lead', 1),\n",
       "  ('leader', 1),\n",
       "  ('leadership', 1),\n",
       "  ('left', 1),\n",
       "  ('less', 1),\n",
       "  ('let', 5),\n",
       "  ('liberty', 2),\n",
       "  ('lie', 2),\n",
       "  ('life', 2),\n",
       "  ('lifetime', 1),\n",
       "  ('likeliest', 1),\n",
       "  ('lincoln', 1),\n",
       "  ('line', 2),\n",
       "  ('listen', 1),\n",
       "  ('little', 2),\n",
       "  ('live', 5),\n",
       "  ('long', 6),\n",
       "  ('longer', 2),\n",
       "  ('look', 2),\n",
       "  ('lot', 1),\n",
       "  ('love', 3),\n",
       "  ('lucky', 1),\n",
       "  ('main', 1),\n",
       "  ('make', 7),\n",
       "  ('malia', 1),\n",
       "  ('man', 5),\n",
       "  ('manager', 1),\n",
       "  ('many', 7),\n",
       "  ('may', 5),\n",
       "  ('measure', 2),\n",
       "  ('meet', 2),\n",
       "  ('message', 1),\n",
       "  ('michelle', 1),\n",
       "  ('million', 2),\n",
       "  ('mind', 1),\n",
       "  ('miss', 1),\n",
       "  ('moine', 1),\n",
       "  ('moment', 2),\n",
       "  ('money', 1),\n",
       "  ('montgomery', 1),\n",
       "  ('month', 2),\n",
       "  ('moon', 1),\n",
       "  ('mortgage', 1),\n",
       "  ('mother', 1),\n",
       "  ('mountain', 1),\n",
       "  ('much', 4),\n",
       "  ('must', 4),\n",
       "  ('myth', 1),\n",
       "  ('nation', 8),\n",
       "  ('national', 1),\n",
       "  ('native', 1),\n",
       "  ('need', 1),\n",
       "  ('never', 5),\n",
       "  ('new', 9),\n",
       "  ('next', 1),\n",
       "  ('night', 1),\n",
       "  ('nixon', 2),\n",
       "  ('not', 4),\n",
       "  ('number', 1),\n",
       "  ('obama', 1),\n",
       "  ('offer', 1),\n",
       "  ('office', 1),\n",
       "  ('old', 2),\n",
       "  ('open', 1),\n",
       "  ('opportunity', 2),\n",
       "  ('organize', 1),\n",
       "  ('other', 1),\n",
       "  ('overcome', 1),\n",
       "  ('palace', 1),\n",
       "  ('palin', 1),\n",
       "  ('parliament', 1),\n",
       "  ('partisanship', 1),\n",
       "  ('partner', 1),\n",
       "  ('party', 3),\n",
       "  ('passion', 1),\n",
       "  ('patriotism', 1),\n",
       "  ('pay', 2),\n",
       "  ('peace', 2),\n",
       "  ('people', 12),\n",
       "  ('perfect', 2),\n",
       "  ('peril', 1),\n",
       "  ('perish', 1),\n",
       "  ('pettiness', 1),\n",
       "  ('pitch', 1),\n",
       "  ('place', 1),\n",
       "  ('plane', 1),\n",
       "  ('planet', 1),\n",
       "  ('plouffe', 1),\n",
       "  ('poison', 1),\n",
       "  ('policy', 1),\n",
       "  ('politic', 2),\n",
       "  ('poor', 1),\n",
       "  ('porch', 1),\n",
       "  ('possible', 1),\n",
       "  ('power', 2),\n",
       "  ('preacher', 1),\n",
       "  ('president', 2),\n",
       "  ('press', 1),\n",
       "  ('problem', 1),\n",
       "  ('progress', 3),\n",
       "  ('promise', 2),\n",
       "  ('promote', 1),\n",
       "  ('prosperity', 1),\n",
       "  ('prove', 2),\n",
       "  ('puppy', 1),\n",
       "  ('purpose', 1),\n",
       "  ('put', 2),\n",
       "  ('question', 1),\n",
       "  ('radio', 1),\n",
       "  ('reach', 1),\n",
       "  ('reaffirm', 1),\n",
       "  ('reason', 1),\n",
       "  ('receive', 1),\n",
       "  ('reclaim', 1),\n",
       "  ('red', 1),\n",
       "  ('reject', 1),\n",
       "  ('reliance', 1),\n",
       "  ('remake', 1),\n",
       "  ('remember', 2),\n",
       "  ('render', 1),\n",
       "  ('renew', 1),\n",
       "  ('repair', 1),\n",
       "  ('republican', 2),\n",
       "  ('resist', 1),\n",
       "  ('resolve', 1),\n",
       "  ('respond', 1),\n",
       "  ('responsibility', 1),\n",
       "  ('restore', 1),\n",
       "  ('rich', 1),\n",
       "  ('rise', 2),\n",
       "  ('risk', 1),\n",
       "  ('road', 2),\n",
       "  ('rock', 1),\n",
       "  ('rode', 1),\n",
       "  ('room', 1),\n",
       "  ('sacrifice', 1),\n",
       "  ('sacrificed', 1),\n",
       "  ('sasha', 1),\n",
       "  ('save', 2),\n",
       "  ('saving', 1),\n",
       "  ('say', 1),\n",
       "  ('scale', 1),\n",
       "  ('school', 2),\n",
       "  ('science', 1),\n",
       "  ('scorch', 1),\n",
       "  ('scranton', 1),\n",
       "  ('screen', 1),\n",
       "  ('security', 1),\n",
       "  ('see', 7),\n",
       "  ('seek', 2),\n",
       "  ('self', 1),\n",
       "  ('selfless', 1),\n",
       "  ('selma', 1),\n",
       "  ('senator_mccain', 1),\n",
       "  ('send', 1),\n",
       "  ('sense', 1),\n",
       "  ('service', 2),\n",
       "  ('setback', 1),\n",
       "  ('shall', 1),\n",
       "  ('share', 2),\n",
       "  ('shore', 1),\n",
       "  ('silence', 1),\n",
       "  ('singular', 1),\n",
       "  ('skin', 1),\n",
       "  ('sky', 1),\n",
       "  ('slavery', 1),\n",
       "  ('sleep', 1),\n",
       "  ('solve', 1),\n",
       "  ('someone', 1),\n",
       "  ('speak', 3),\n",
       "  ('spirit', 2),\n",
       "  ('stand', 4),\n",
       "  ('start', 2),\n",
       "  ('state', 3),\n",
       "  ('steep', 1),\n",
       "  ('still', 4),\n",
       "  ('story', 2),\n",
       "  ('straight', 1),\n",
       "  ('strained', 1),\n",
       "  ('stranger', 1),\n",
       "  ('strategist', 1),\n",
       "  ('street', 2),\n",
       "  ('strength', 2),\n",
       "  ('stretch', 1),\n",
       "  ('struggle', 1),\n",
       "  ('suffer', 1),\n",
       "  ('sum', 1),\n",
       "  ('summon', 1),\n",
       "  ('support', 3),\n",
       "  ('task', 1),\n",
       "  ('teach', 1),\n",
       "  ('team', 1),\n",
       "  ('tear', 1),\n",
       "  ('tell', 6),\n",
       "  ('temptation', 1),\n",
       "  ('term', 1),\n",
       "  ('thank', 2),\n",
       "  ('thing', 3),\n",
       "  ('think', 1),\n",
       "  ('threat', 1),\n",
       "  ('threaten', 1),\n",
       "  ('thrive', 1),\n",
       "  ('time', 9),\n",
       "  ('timeless', 1),\n",
       "  ('tomorrow', 2),\n",
       "  ('tonight', 13),\n",
       "  ('touch', 2),\n",
       "  ('train', 1),\n",
       "  ('true', 2),\n",
       "  ('truly', 1),\n",
       "  ('truth', 1),\n",
       "  ('tyranny', 1),\n",
       "  ('understand', 1),\n",
       "  ('union', 1),\n",
       "  ('united_state', 1),\n",
       "  ('united_states', 2),\n",
       "  ('unity', 1),\n",
       "  ('unyielde', 1),\n",
       "  ('unyielding', 1),\n",
       "  ('value', 2),\n",
       "  ('vice_president', 1),\n",
       "  ('victory', 4),\n",
       "  ('voice', 4),\n",
       "  ('volunteer', 1),\n",
       "  ('vote', 3),\n",
       "  ('wait', 1),\n",
       "  ('wake', 1),\n",
       "  ('wall', 1),\n",
       "  ('wall_street', 1),\n",
       "  ('want', 1),\n",
       "  ('war', 1),\n",
       "  ('washington', 1),\n",
       "  ('watch', 2),\n",
       "  ('way', 2),\n",
       "  ('wealth', 1),\n",
       "  ('well', 2),\n",
       "  ('white', 1),\n",
       "  ('white_house', 2),\n",
       "  ('win', 1),\n",
       "  ('winter', 1),\n",
       "  ('witness', 1),\n",
       "  ('woman', 5),\n",
       "  ('wonder', 3),\n",
       "  ('work', 5),\n",
       "  ('world', 5),\n",
       "  ('would', 2),\n",
       "  ('year', 6),\n",
       "  ('young', 3)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA model\n",
    "Save the model in `./models/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "lda_model.save('./models/'+PRES+'-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Keywords in the topics\n",
    "Save them to a txt in `./outputs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"./output/\"+PRES+\".txt\",\"w\")\n",
    "for topic in lda_model.print_topics():\n",
    "    file.write(str(topic[0]) + \": \" + topic[1])\n",
    "file.close()\n",
    "\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.456062006400461\n",
      "\n",
      "Coherence Score:  0.3547375540499792\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "gov_filter = set(['year','know','take','go','say','must','today','new', 'good','year','want','need','government','people','country','great', 'united_state','country','man','nation','law', 'america','american','time','tonight','make','help','may','would','congress','thereof','care','rule','place','bill','duty','amount'])\n",
    "def perPresidentAnalysis(PRES):\n",
    "    docs = df[df['pres'] == PRES]\n",
    "    if 0 in docs.shape: return\n",
    "    data = docs.speech.values.tolist()\n",
    "    data = [re.sub('\\n+', ' ', sent) for sent in data]\n",
    "    data = [re.sub(r\"\\<.*\\>\",\" \",sent) for sent in data]\n",
    "    data_words = list(sent_to_words(data))\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    \n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    \n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    \n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "    \n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    lda_model.save('./models/'+PRES+'-model')\n",
    "    file = open(\"./output/\"+PRES+\".txt\",\"w\")\n",
    "    for topic in lda_model.print_topics():\n",
    "        topics = re.compile('[0-9 .+*\"]*').split(topic[1])\n",
    "#         max_weight=-1\n",
    "#         max_topic=\"\"\n",
    "#         for i,word, in enumerate(topics):\n",
    "#             if isfloat(word):\n",
    "#                 if max_weight < float(word) and word not in seen and word not in gov_filter:\n",
    "#                     max_weight = float(word)\n",
    "#                     max_topic = topics[i+1]\n",
    "#         seen.add(max_topic)\n",
    "#         file.write(max_topic +\" \")\n",
    "        for word in topics:\n",
    "            if word not in gov_filter:\n",
    "                file.write(word + \" \")\n",
    "    file.close()\n",
    "    doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing President 1 of 45 --- Done\n",
      "Analyzing President 2 of 45 --- Done\n",
      "Analyzing President 3 of 45 --- Done\n",
      "Analyzing President 4 of 45 --- Done\n",
      "Analyzing President 5 of 45 --- Done\n",
      "Analyzing President 6 of 45 --- Done\n",
      "Analyzing President 7 of 45 --- Done\n",
      "Analyzing President 8 of 45 --- Done\n",
      "Analyzing President 9 of 45 --- Done\n",
      "Analyzing President 10 of 45 --- Done\n",
      "Analyzing President 11 of 45 --- Done\n",
      "Analyzing President 12 of 45 --- Done\n",
      "Analyzing President 13 of 45 --- Done\n",
      "Analyzing President 14 of 45 --- Done\n",
      "Analyzing President 15 of 45 --- Done\n",
      "Analyzing President 16 of 45 --- Done\n",
      "Analyzing President 17 of 45 --- Done\n",
      "Analyzing President 18 of 45 --- Done\n",
      "Analyzing President 19 of 45 --- Done\n",
      "Analyzing President 20 of 45 --- Done\n",
      "Analyzing President 21 of 45 --- Done\n",
      "Analyzing President 22 of 45 --- Done\n",
      "Analyzing President 23 of 45 --- Done\n",
      "Analyzing President 24 of 45 --- Done\n",
      "Analyzing President 25 of 45 --- Done\n",
      "Analyzing President 26 of 45 --- Done\n",
      "Analyzing President 27 of 45 --- Done\n",
      "Analyzing President 28 of 45 --- Done\n",
      "Analyzing President 29 of 45 --- Done\n",
      "Analyzing President 30 of 45 --- Done\n",
      "Analyzing President 31 of 45 --- Done\n",
      "Analyzing President 32 of 45 --- Done\n",
      "Analyzing President 33 of 45 --- Done\n",
      "Analyzing President 34 of 45 --- Done\n",
      "Analyzing President 35 of 45 --- Done\n",
      "Analyzing President 36 of 45 --- Done\n",
      "Analyzing President 37 of 45 --- Done\n",
      "Analyzing President 38 of 45 --- Done\n",
      "Analyzing President 39 of 45 --- Done\n",
      "Analyzing President 40 of 45 --- Done\n",
      "Analyzing President 41 of 45 --- Done\n",
      "Analyzing President 42 of 45 --- Done\n",
      "Analyzing President 43 of 45 --- Done\n",
      "Analyzing President 44 of 45 --- Done\n",
      "Analyzing President 45 of 45 --- Done\n"
     ]
    }
   ],
   "source": [
    "preses = list(df.pres.unique())\n",
    "# print(preses)\n",
    "for president in preses:\n",
    "    print('Analyzing President',preses.index(president)+1,'of',len(preses),'---',end=\" \")\n",
    "    perPresidentAnalysis(president)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4      0.018859 -0.049189       1        1  50.992073\n",
       "1      0.084270  0.030464       2        1  27.396254\n",
       "2     -0.025640 -0.107436       3        1  11.169715\n",
       "0      0.049418  0.065881       4        1   5.923071\n",
       "3     -0.126906  0.060280       5        1   4.518895, topic_info=     Category        Freq         Term       Total  loglift  logprob\n",
       "term                                                                \n",
       "1603  Default   54.000000    insurance   54.000000  30.0000  30.0000\n",
       "211   Default   81.000000         iraq   81.000000  29.0000  29.0000\n",
       "422   Default   46.000000        think   46.000000  28.0000  28.0000\n",
       "171   Default  106.000000          get  106.000000  27.0000  27.0000\n",
       "272   Default  113.000000       nation  113.000000  26.0000  26.0000\n",
       "420   Default   50.000000        thank   50.000000  25.0000  25.0000\n",
       "655   Default   50.000000  health_care   50.000000  24.0000  24.0000\n",
       "469   Default   91.000000        would   91.000000  23.0000  23.0000\n",
       "302   Default  223.000000       people  223.000000  22.0000  22.0000\n",
       "747   Default   63.000000         plan   63.000000  21.0000  21.0000\n",
       "1019  Default   38.000000    community   38.000000  20.0000  20.0000\n",
       "468   Default  103.000000        world  103.000000  19.0000  19.0000\n",
       "454   Default   79.000000          war   79.000000  18.0000  18.0000\n",
       "453   Default   60.000000         want   60.000000  17.0000  17.0000\n",
       "138   Default   59.000000          end   59.000000  16.0000  16.0000\n",
       "1031  Default   28.000000     coverage   28.000000  15.0000  15.0000\n",
       "277   Default  117.000000          new  117.000000  14.0000  14.0000\n",
       "11    Default  188.000000      america  188.000000  13.0000  13.0000\n",
       "270   Default   87.000000         must   87.000000  12.0000  12.0000\n",
       "300   Default   57.000000          pay   57.000000  11.0000  11.0000\n",
       "372   Default   79.000000          see   79.000000  10.0000  10.0000\n",
       "397   Default   47.000000        state   47.000000   9.0000   9.0000\n",
       "787   Default   52.000000       reform   52.000000   8.0000   8.0000\n",
       "1383  Default   44.000000        troop   44.000000   7.0000   7.0000\n",
       "243   Default   20.000000          lot   20.000000   6.0000   6.0000\n",
       "83    Default  103.000000         come  103.000000   5.0000   5.0000\n",
       "1178  Default   42.000000        issue   42.000000   4.0000   4.0000\n",
       "173   Default  103.000000           go  103.000000   3.0000   3.0000\n",
       "12    Default  235.000000     american  235.000000   2.0000   2.0000\n",
       "421   Default   54.000000        thing   54.000000   1.0000   1.0000\n",
       "...       ...         ...          ...         ...      ...      ...\n",
       "1434   Topic5    3.027156         folk   10.032898   1.8987  -5.9467\n",
       "185    Topic5    5.131604       happen   28.132002   1.3954  -5.4189\n",
       "1019   Topic5    5.938804    community   38.590500   1.2254  -5.2728\n",
       "1993   Topic5    2.437993        level    6.865205   2.0616  -6.1632\n",
       "331    Topic5    4.596547     question   24.887161   1.4079  -5.5290\n",
       "238    Topic5    2.926334       little   10.235081   1.8448  -5.9806\n",
       "1187   Topic5    3.599725         kind   15.701039   1.6240  -5.7735\n",
       "397    Topic5    5.866867        state   47.101357   1.0139  -5.2850\n",
       "14     Topic5    3.091918       answer   12.472239   1.7022  -5.9256\n",
       "945    Topic5    2.438393     actually    7.615138   1.9581  -6.1630\n",
       "1573   Topic5    1.853091       excuse    4.268091   2.2626  -6.4375\n",
       "1178   Topic5    5.384728        issue   42.632111   1.0279  -5.3708\n",
       "993    Topic5    3.025665         case   12.819797   1.6530  -5.9472\n",
       "171    Topic5    7.326882          get  106.662903   0.4188  -5.0628\n",
       "453    Topic5    5.802401         want   60.540787   0.7519  -5.2961\n",
       "372    Topic5    6.138360          see   79.185806   0.5397  -5.2398\n",
       "421    Topic5    5.254580        thing   54.352562   0.7605  -5.3952\n",
       "269    Topic5    4.684480         much   43.834240   0.8607  -5.5101\n",
       "469    Topic5    6.032225        would   91.722977   0.3752  -5.2572\n",
       "457    Topic5    4.902194          way   57.644154   0.6323  -5.4647\n",
       "459    Topic5    5.372782         well   78.947411   0.4095  -5.3730\n",
       "302    Topic5    7.191041       people  223.990067  -0.3419  -5.0815\n",
       "691    Topic5    4.222559          law   39.686630   0.8563  -5.6139\n",
       "252    Topic5    4.585198          may   56.980953   0.5770  -5.5315\n",
       "173    Topic5    5.163665           go  103.255455   0.1013  -5.4127\n",
       "229    Topic5    4.472112          let   95.678375   0.0338  -5.5565\n",
       "1359   Topic5    3.619133         talk   33.219738   0.8800  -5.7681\n",
       "92     Topic5    3.939197      country  129.205719  -0.3935  -5.6834\n",
       "467    Topic5    4.028462         work  183.807388  -0.7236  -5.6610\n",
       "83     Topic5    3.713602         come  103.256775  -0.2283  -5.7423\n",
       "\n",
       "[396 rows x 6 columns], token_table=      Topic      Freq              Term\n",
       "term                                   \n",
       "476       1  0.786377               act\n",
       "476       2  0.122871               act\n",
       "476       3  0.073723               act\n",
       "476       4  0.024574               act\n",
       "945       1  0.393952          actually\n",
       "945       2  0.262635          actually\n",
       "945       3  0.131317          actually\n",
       "945       5  0.262635          actually\n",
       "950       1  0.370491        affordable\n",
       "950       2  0.074098        affordable\n",
       "950       3  0.518688        affordable\n",
       "1723      2  0.101560  african_american\n",
       "1723      5  0.812478  african_american\n",
       "484       1  0.413940              also\n",
       "484       2  0.413940              also\n",
       "484       3  0.120176              also\n",
       "484       4  0.026706              also\n",
       "484       5  0.026706              also\n",
       "3372      5  0.514927       altercation\n",
       "485       1  0.329484          ambition\n",
       "485       4  0.329484          ambition\n",
       "11        1  0.515207           america\n",
       "11        2  0.403667           america\n",
       "11        3  0.026557           america\n",
       "11        4  0.047803           america\n",
       "11        5  0.005311           america\n",
       "12        1  0.595356          american\n",
       "12        2  0.255152          american\n",
       "12        3  0.110566          american\n",
       "12        4  0.034020          american\n",
       "...     ...       ...               ...\n",
       "2921      3  0.853710              wife\n",
       "929       1  0.329428       willingness\n",
       "929       4  0.329428       willingness\n",
       "463       3  0.286857            winter\n",
       "463       4  0.286857            winter\n",
       "463       5  0.286857            winter\n",
       "465       1  0.378876             woman\n",
       "465       2  0.457265             woman\n",
       "465       3  0.091453             woman\n",
       "465       4  0.052259             woman\n",
       "465       5  0.013065             woman\n",
       "467       1  0.658298              work\n",
       "467       2  0.190417              work\n",
       "467       3  0.092488              work\n",
       "467       4  0.038083              work\n",
       "467       5  0.021762              work\n",
       "2664      1  0.938854         workforce\n",
       "468       1  0.433485             world\n",
       "468       2  0.500916             world\n",
       "468       4  0.067431             world\n",
       "469       1  0.523315             would\n",
       "469       2  0.163536             would\n",
       "469       3  0.228950             would\n",
       "469       4  0.021805             would\n",
       "469       5  0.065414             would\n",
       "470       1  0.695036              year\n",
       "470       2  0.172274              year\n",
       "470       3  0.100988              year\n",
       "470       4  0.029702              year\n",
       "470       5  0.005940              year\n",
       "\n",
       "[766 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 2, 3, 1, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
